Echoing to file start
Save config dict to file
Set seed to 1234
Start loading data
End loading data
Generate Datalaoder
Generate Datalaoder end
Start training
step=0 loss=4.2006 running loss=4.1965 , time=6.8980 expected time=607.0245
Epoch 1, train_loss=3.9093, train_acc=0.0250 val_loss=4.1098,  val_acc=0.0266
step=0 loss=3.5863 running loss=3.5822 , time=1.2097 expected time=106.4554
Epoch 2, train_loss=3.6455, train_acc=0.0407 val_loss=4.0262,  val_acc=0.0465
step=0 loss=3.6411 running loss=3.6358 , time=1.2038 expected time=105.9324
Epoch 3, train_loss=3.4731, train_acc=0.0764 val_loss=4.0007,  val_acc=0.0432
step=0 loss=3.2669 running loss=3.2604 , time=1.2004 expected time=105.6326
Epoch 4, train_loss=3.3176, train_acc=0.0971 val_loss=3.2977,  val_acc=0.0797
step=0 loss=3.0905 running loss=3.0828 , time=1.2071 expected time=106.2268
Epoch 5, train_loss=3.1667, train_acc=0.1236 val_loss=4.1264,  val_acc=0.0598
step=0 loss=3.2756 running loss=3.2666 , time=1.2032 expected time=105.8775
Epoch 6, train_loss=3.0427, train_acc=0.1486 val_loss=3.0503,  val_acc=0.1794
step=0 loss=2.6014 running loss=2.5909 , time=1.2020 expected time=105.7740
Epoch 7, train_loss=2.9672, train_acc=0.1679 val_loss=3.1204,  val_acc=0.1528
step=0 loss=2.7858 running loss=2.7743 , time=1.2078 expected time=106.2902
Epoch 8, train_loss=2.8847, train_acc=0.1786 val_loss=3.0753,  val_acc=0.1395
step=0 loss=3.4392 running loss=3.4269 , time=1.2007 expected time=105.6635
Epoch 9, train_loss=2.8070, train_acc=0.1950 val_loss=2.9385,  val_acc=0.2060
step=0 loss=2.5193 running loss=2.5061 , time=1.2045 expected time=105.9985
Epoch 10, train_loss=2.6965, train_acc=0.2071 val_loss=2.9646,  val_acc=0.2093
step=0 loss=2.4261 running loss=2.4121 , time=1.2099 expected time=106.4748
Epoch 11, train_loss=2.6169, train_acc=0.2171 val_loss=2.8063,  val_acc=0.1860
step=0 loss=2.4386 running loss=2.4240 , time=1.2013 expected time=105.7138
Epoch 12, train_loss=2.5682, train_acc=0.2421 val_loss=2.6897,  val_acc=0.2492
step=0 loss=2.4696 running loss=2.4545 , time=1.2168 expected time=107.0793
Epoch 13, train_loss=2.5270, train_acc=0.2586 val_loss=2.6399,  val_acc=0.2326
step=0 loss=2.3830 running loss=2.3676 , time=1.1981 expected time=105.4366
Epoch 14, train_loss=2.4196, train_acc=0.2686 val_loss=2.6357,  val_acc=0.2658
step=0 loss=2.3719 running loss=2.3559 , time=1.2032 expected time=105.8857
Epoch 15, train_loss=2.3783, train_acc=0.3007 val_loss=2.7200,  val_acc=0.2326
step=0 loss=2.0080 running loss=1.9916 , time=1.2210 expected time=107.4457
Epoch 16, train_loss=2.3443, train_acc=0.2986 val_loss=2.6107,  val_acc=0.2824
step=0 loss=2.2720 running loss=2.2553 , time=1.2087 expected time=106.3675
Epoch 17, train_loss=2.2983, train_acc=0.3164 val_loss=2.5219,  val_acc=0.2990
step=0 loss=1.5743 running loss=1.5573 , time=1.2080 expected time=106.3012
Epoch 18, train_loss=2.2038, train_acc=0.3279 val_loss=3.9223,  val_acc=0.1262
step=0 loss=2.2001 running loss=2.1826 , time=1.2096 expected time=106.4461
Epoch 19, train_loss=2.2026, train_acc=0.3307 val_loss=2.5094,  val_acc=0.2924
step=0 loss=1.8148 running loss=1.7970 , time=1.2077 expected time=106.2744
Epoch 20, train_loss=2.1067, train_acc=0.3657 val_loss=2.5731,  val_acc=0.2890
step=0 loss=2.2394 running loss=2.2213 , time=1.2139 expected time=106.8213
Epoch 21, train_loss=2.0754, train_acc=0.3643 val_loss=2.7280,  val_acc=0.2525
step=0 loss=1.8540 running loss=1.8356 , time=1.2100 expected time=106.4788
Epoch 22, train_loss=2.0153, train_acc=0.3864 val_loss=2.5580,  val_acc=0.2890
step=0 loss=1.5050 running loss=1.4862 , time=1.2137 expected time=106.8071
Epoch 23, train_loss=1.9526, train_acc=0.3943 val_loss=2.4883,  val_acc=0.2957
step=0 loss=2.2694 running loss=2.2503 , time=1.2082 expected time=106.3247
Epoch 24, train_loss=1.8926, train_acc=0.4279 val_loss=2.4069,  val_acc=0.3355
step=0 loss=1.7606 running loss=1.7412 , time=1.2162 expected time=107.0275
Epoch 25, train_loss=1.8622, train_acc=0.4286 val_loss=2.4436,  val_acc=0.3654
step=0 loss=1.6525 running loss=1.6328 , time=1.2084 expected time=106.3360
Epoch 26, train_loss=1.7783, train_acc=0.4500 val_loss=2.4737,  val_acc=0.3455
step=0 loss=1.7846 running loss=1.7645 , time=1.2112 expected time=106.5835
Epoch 27, train_loss=1.6899, train_acc=0.4821 val_loss=2.5554,  val_acc=0.3555
step=0 loss=1.9692 running loss=1.9488 , time=1.2116 expected time=106.6181
Epoch 28, train_loss=1.6497, train_acc=0.5100 val_loss=2.6640,  val_acc=0.3289
step=0 loss=1.8057 running loss=1.7851 , time=1.2073 expected time=106.2407
Epoch 29, train_loss=1.6208, train_acc=0.5079 val_loss=2.3687,  val_acc=0.3588
step=0 loss=1.5327 running loss=1.5118 , time=1.2077 expected time=106.2784
Epoch 30, train_loss=1.5583, train_acc=0.5271 val_loss=2.4163,  val_acc=0.3488
step=0 loss=1.5571 running loss=1.5362 , time=1.2059 expected time=106.1151
Epoch 31, train_loss=1.5676, train_acc=0.5193 val_loss=2.5242,  val_acc=0.3688
step=0 loss=1.4767 running loss=1.4556 , time=1.2101 expected time=106.4924
Epoch 32, train_loss=1.4612, train_acc=0.5629 val_loss=2.6021,  val_acc=0.3787
step=0 loss=1.2471 running loss=1.2260 , time=1.2093 expected time=106.4148
Epoch 33, train_loss=1.3970, train_acc=0.5700 val_loss=2.4510,  val_acc=0.4153
step=0 loss=1.8771 running loss=1.8557 , time=1.2064 expected time=106.1659
Epoch 34, train_loss=1.3711, train_acc=0.5750 val_loss=2.4689,  val_acc=0.3721
step=0 loss=1.5217 running loss=1.5002 , time=1.2080 expected time=106.3036
Epoch 35, train_loss=1.3397, train_acc=0.5850 val_loss=2.4374,  val_acc=0.4153
step=0 loss=1.4526 running loss=1.4310 , time=1.2077 expected time=106.2798
Epoch 36, train_loss=1.2952, train_acc=0.5979 val_loss=2.4414,  val_acc=0.4252
step=0 loss=1.5857 running loss=1.5639 , time=1.2066 expected time=106.1806
Epoch 37, train_loss=1.2404, train_acc=0.6157 val_loss=2.5064,  val_acc=0.4153
step=0 loss=0.9096 running loss=0.8877 , time=1.2091 expected time=106.3980
Epoch 38, train_loss=1.2048, train_acc=0.6236 val_loss=2.4013,  val_acc=0.4219
step=0 loss=0.9806 running loss=0.9586 , time=1.2084 expected time=106.3405
Epoch 39, train_loss=1.1559, train_acc=0.6329 val_loss=2.4642,  val_acc=0.4120
step=0 loss=2.5658 running loss=2.5436 , time=1.2079 expected time=106.2937
Epoch 40, train_loss=1.1397, train_acc=0.6543 val_loss=2.6152,  val_acc=0.3953
step=0 loss=1.2047 running loss=1.1826 , time=1.2118 expected time=106.6353
Epoch 41, train_loss=1.1031, train_acc=0.6729 val_loss=2.5963,  val_acc=0.4086
step=0 loss=1.2007 running loss=1.1784 , time=1.2074 expected time=106.2491
Epoch 42, train_loss=1.0454, train_acc=0.6957 val_loss=2.5527,  val_acc=0.4186
step=0 loss=1.1059 running loss=1.0836 , time=1.2184 expected time=107.2227
Epoch 43, train_loss=1.0042, train_acc=0.6943 val_loss=2.6371,  val_acc=0.4286
step=0 loss=0.9078 running loss=0.8854 , time=1.2090 expected time=106.3906
Epoch 44, train_loss=0.9932, train_acc=0.7029 val_loss=2.6329,  val_acc=0.4120
step=0 loss=0.9154 running loss=0.8929 , time=1.2067 expected time=106.1895
Epoch 45, train_loss=0.9623, train_acc=0.7021 val_loss=2.6953,  val_acc=0.4252
step=0 loss=1.1901 running loss=1.1675 , time=1.2120 expected time=106.6556
Epoch 46, train_loss=0.9499, train_acc=0.7079 val_loss=2.5736,  val_acc=0.4286
step=0 loss=0.7465 running loss=0.7240 , time=1.2068 expected time=106.1943
Epoch 47, train_loss=0.9267, train_acc=0.7214 val_loss=2.6510,  val_acc=0.4020
step=0 loss=1.0792 running loss=1.0566 , time=1.2077 expected time=106.2794
Epoch 48, train_loss=0.9009, train_acc=0.7257 val_loss=2.6335,  val_acc=0.4352
step=0 loss=0.7676 running loss=0.7450 , time=1.2066 expected time=106.1846
Epoch 49, train_loss=0.9116, train_acc=0.7293 val_loss=2.6823,  val_acc=0.4153
step=0 loss=0.8193 running loss=0.7966 , time=1.2092 expected time=106.4130
Epoch 50, train_loss=0.8790, train_acc=0.7329 val_loss=2.7423,  val_acc=0.4086
step=0 loss=0.9061 running loss=0.8834 , time=1.2082 expected time=106.3194
Epoch 51, train_loss=0.8600, train_acc=0.7393 val_loss=2.6674,  val_acc=0.4286
step=0 loss=0.5875 running loss=0.5648 , time=1.2075 expected time=106.2621
Epoch 52, train_loss=0.8618, train_acc=0.7471 val_loss=2.7330,  val_acc=0.4086
step=0 loss=0.7440 running loss=0.7213 , time=1.2084 expected time=106.3401
Epoch 53, train_loss=0.8172, train_acc=0.7614 val_loss=2.7524,  val_acc=0.4219
step=0 loss=0.4120 running loss=0.3893 , time=1.2071 expected time=106.2249
Epoch 54, train_loss=0.8516, train_acc=0.7450 val_loss=2.7513,  val_acc=0.4086
step=0 loss=0.5742 running loss=0.5514 , time=1.2080 expected time=106.3000
Epoch 55, train_loss=0.8500, train_acc=0.7464 val_loss=2.7041,  val_acc=0.4086
step=0 loss=0.6612 running loss=0.6384 , time=1.2084 expected time=106.3374
Epoch 56, train_loss=0.8239, train_acc=0.7521 val_loss=2.7405,  val_acc=0.4252
step=0 loss=0.9997 running loss=0.9769 , time=1.2069 expected time=106.2069
Epoch 57, train_loss=0.8352, train_acc=0.7421 val_loss=2.7623,  val_acc=0.4219
step=0 loss=1.0603 running loss=1.0376 , time=1.2071 expected time=106.2254
Epoch 58, train_loss=0.7988, train_acc=0.7586 val_loss=2.7089,  val_acc=0.4219
step=0 loss=0.6240 running loss=0.6012 , time=1.2067 expected time=106.1893
Epoch 59, train_loss=0.7950, train_acc=0.7679 val_loss=2.6980,  val_acc=0.4252
step=0 loss=0.7680 running loss=0.7452 , time=1.2055 expected time=106.0834
Epoch 60, train_loss=0.8029, train_acc=0.7614 val_loss=2.7977,  val_acc=0.4086
Evaluate on test set
