Echoing to file start
Save config dict to file
Set seed to 1234
Start loading data
End loading data
Generate Datalaoder
Generate Datalaoder end
Start training
step=0 loss=4.2006 running loss=4.1965 , time=7.1425 expected time=628.5395
Epoch 1, train_loss=3.8396, train_acc=0.0400 val_loss=4.2849,  val_acc=0.0233
step=0 loss=3.8818 running loss=3.8730 , time=1.3459 expected time=118.4351
Epoch 2, train_loss=3.7103, train_acc=0.0507 val_loss=4.0135,  val_acc=0.0332
step=0 loss=3.5096 running loss=3.4929 , time=1.2959 expected time=114.0419
Epoch 3, train_loss=3.5550, train_acc=0.0643 val_loss=4.0615,  val_acc=0.0365
step=0 loss=3.6234 running loss=3.6019 , time=1.3209 expected time=116.2352
Epoch 4, train_loss=3.4591, train_acc=0.0857 val_loss=3.3424,  val_acc=0.0831
step=0 loss=2.9846 running loss=2.9575 , time=1.2548 expected time=110.4234
Epoch 5, train_loss=3.3333, train_acc=0.1029 val_loss=3.7479,  val_acc=0.0498
step=0 loss=3.1094 running loss=3.0778 , time=1.2580 expected time=110.7024
Epoch 6, train_loss=3.2289, train_acc=0.1100 val_loss=4.1183,  val_acc=0.0598
step=0 loss=3.8549 running loss=3.8204 , time=1.2441 expected time=109.4798
Epoch 7, train_loss=3.1707, train_acc=0.1436 val_loss=3.4975,  val_acc=0.0864
step=0 loss=3.0367 running loss=2.9997 , time=1.2507 expected time=110.0608
Epoch 8, train_loss=3.0569, train_acc=0.1671 val_loss=3.1584,  val_acc=0.1229
step=0 loss=3.6018 running loss=3.5621 , time=1.2515 expected time=110.1278
Epoch 9, train_loss=2.8972, train_acc=0.1943 val_loss=3.7487,  val_acc=0.1429
step=0 loss=2.7376 running loss=2.6946 , time=1.2655 expected time=111.3626
Epoch 10, train_loss=2.7757, train_acc=0.2036 val_loss=3.1238,  val_acc=0.1329
step=0 loss=2.6936 running loss=2.6457 , time=1.7052 expected time=150.0602
Epoch 11, train_loss=2.7196, train_acc=0.2371 val_loss=3.1193,  val_acc=0.1595
step=0 loss=2.5769 running loss=2.5259 , time=1.7166 expected time=151.0596
Epoch 12, train_loss=2.6037, train_acc=0.2557 val_loss=3.3162,  val_acc=0.1262
step=0 loss=2.4345 running loss=2.3798 , time=1.7021 expected time=149.7891
Epoch 13, train_loss=2.5314, train_acc=0.2793 val_loss=2.7093,  val_acc=0.2359
step=0 loss=2.3663 running loss=2.3096 , time=1.7288 expected time=152.1357
Epoch 14, train_loss=2.3987, train_acc=0.2843 val_loss=3.3904,  val_acc=0.1960
step=0 loss=2.1638 running loss=2.1060 , time=1.6740 expected time=147.3139
Epoch 15, train_loss=2.3116, train_acc=0.3343 val_loss=2.8545,  val_acc=0.2425
step=0 loss=1.8720 running loss=1.8127 , time=1.7323 expected time=152.4418
Epoch 16, train_loss=2.3398, train_acc=0.3250 val_loss=2.7406,  val_acc=0.2558
step=0 loss=2.4672 running loss=2.4065 , time=1.8691 expected time=164.4795
Epoch 17, train_loss=2.2487, train_acc=0.3521 val_loss=3.6151,  val_acc=0.1794
step=0 loss=1.9206 running loss=1.8582 , time=1.8327 expected time=161.2794
Epoch 18, train_loss=2.2076, train_acc=0.3764 val_loss=2.6370,  val_acc=0.2525
step=0 loss=2.4311 running loss=2.3697 , time=1.6330 expected time=143.7021
Epoch 19, train_loss=2.1270, train_acc=0.3836 val_loss=2.4515,  val_acc=0.3522
step=0 loss=1.7498 running loss=1.6868 , time=1.7024 expected time=149.8100
Epoch 20, train_loss=2.0389, train_acc=0.4143 val_loss=2.4831,  val_acc=0.3223
step=0 loss=1.8609 running loss=1.7980 , time=1.7173 expected time=151.1244
Epoch 21, train_loss=1.9943, train_acc=0.4136 val_loss=2.4243,  val_acc=0.2857
step=0 loss=1.7724 running loss=1.7087 , time=1.7115 expected time=150.6138
Epoch 22, train_loss=1.9536, train_acc=0.4136 val_loss=2.0978,  val_acc=0.3953
step=0 loss=1.5195 running loss=1.4536 , time=1.6774 expected time=147.6128
Epoch 23, train_loss=1.8886, train_acc=0.4493 val_loss=2.2350,  val_acc=0.4186
step=0 loss=1.8171 running loss=1.7522 , time=1.7238 expected time=151.6965
Epoch 24, train_loss=1.7988, train_acc=0.4700 val_loss=2.1203,  val_acc=0.4086
step=0 loss=2.3855 running loss=2.3201 , time=1.7010 expected time=149.6842
Epoch 25, train_loss=1.7191, train_acc=0.4964 val_loss=2.0957,  val_acc=0.4186
step=0 loss=1.7823 running loss=1.7164 , time=1.7205 expected time=151.4036
Epoch 26, train_loss=1.7090, train_acc=0.4893 val_loss=2.2089,  val_acc=0.3721
step=0 loss=1.3327 running loss=1.2666 , time=1.6916 expected time=148.8592
Epoch 27, train_loss=1.6956, train_acc=0.5100 val_loss=2.5504,  val_acc=0.2757
step=0 loss=1.8597 running loss=1.7940 , time=1.7061 expected time=150.1355
Epoch 28, train_loss=1.6106, train_acc=0.5257 val_loss=1.9834,  val_acc=0.4219
step=0 loss=1.6954 running loss=1.6295 , time=1.7095 expected time=150.4346
Epoch 29, train_loss=1.5712, train_acc=0.5321 val_loss=2.0954,  val_acc=0.4086
step=0 loss=1.5352 running loss=1.4698 , time=1.7152 expected time=150.9401
Epoch 30, train_loss=1.5895, train_acc=0.5264 val_loss=1.9903,  val_acc=0.4286
step=0 loss=0.8389 running loss=0.7742 , time=1.7211 expected time=151.4540
Epoch 31, train_loss=1.4291, train_acc=0.5686 val_loss=1.8666,  val_acc=0.4452
step=0 loss=1.2867 running loss=1.2225 , time=1.6726 expected time=147.1871
Epoch 32, train_loss=1.3604, train_acc=0.5821 val_loss=1.7430,  val_acc=0.5083
step=0 loss=1.5384 running loss=1.4747 , time=1.7089 expected time=150.3867
Epoch 33, train_loss=1.2791, train_acc=0.6129 val_loss=1.6915,  val_acc=0.4983
step=0 loss=1.9905 running loss=1.9267 , time=1.7058 expected time=150.1142
Epoch 34, train_loss=1.2984, train_acc=0.6071 val_loss=1.7719,  val_acc=0.5282
step=0 loss=1.4277 running loss=1.3638 , time=1.7992 expected time=158.3295
Epoch 35, train_loss=1.2070, train_acc=0.6321 val_loss=1.6713,  val_acc=0.5382
step=0 loss=1.2822 running loss=1.2185 , time=1.8314 expected time=161.1636
Epoch 36, train_loss=1.1222, train_acc=0.6557 val_loss=1.7572,  val_acc=0.5249
step=0 loss=0.8280 running loss=0.7646 , time=1.8163 expected time=159.8304
Epoch 37, train_loss=1.1666, train_acc=0.6457 val_loss=1.6875,  val_acc=0.5083
step=0 loss=0.9781 running loss=0.9155 , time=1.8044 expected time=158.7901
Epoch 38, train_loss=1.0763, train_acc=0.6771 val_loss=1.7188,  val_acc=0.5083
step=0 loss=0.7472 running loss=0.6845 , time=1.6735 expected time=147.2704
Epoch 39, train_loss=1.0315, train_acc=0.6786 val_loss=1.7100,  val_acc=0.5216
step=0 loss=1.4412 running loss=1.3782 , time=1.6857 expected time=148.3438
Epoch 40, train_loss=0.9929, train_acc=0.6964 val_loss=1.5580,  val_acc=0.5581
step=0 loss=1.0858 running loss=1.0232 , time=1.6669 expected time=146.6890
Epoch 41, train_loss=0.9600, train_acc=0.6943 val_loss=1.5181,  val_acc=0.6146
step=0 loss=0.8116 running loss=0.7498 , time=1.9496 expected time=171.5630
Epoch 42, train_loss=0.9191, train_acc=0.7207 val_loss=1.5852,  val_acc=0.5681
step=0 loss=1.0211 running loss=0.9601 , time=1.6764 expected time=147.5229
Epoch 43, train_loss=0.9176, train_acc=0.7079 val_loss=1.6775,  val_acc=0.5216
step=0 loss=1.0394 running loss=0.9788 , time=1.6735 expected time=147.2712
Epoch 44, train_loss=0.7992, train_acc=0.7536 val_loss=1.6063,  val_acc=0.5548
step=0 loss=0.6678 running loss=0.6078 , time=1.6728 expected time=147.2052
Epoch 45, train_loss=0.8883, train_acc=0.7279 val_loss=1.6547,  val_acc=0.5282
step=0 loss=0.7011 running loss=0.6420 , time=1.6868 expected time=148.4394
Epoch 46, train_loss=0.8004, train_acc=0.7457 val_loss=1.4718,  val_acc=0.5947
step=0 loss=0.6597 running loss=0.6008 , time=1.6879 expected time=148.5362
Epoch 47, train_loss=0.6984, train_acc=0.7879 val_loss=1.4678,  val_acc=0.6047
step=0 loss=0.5096 running loss=0.4511 , time=1.6877 expected time=148.5185
Epoch 48, train_loss=0.6896, train_acc=0.7786 val_loss=1.4808,  val_acc=0.6445
step=0 loss=0.5554 running loss=0.4974 , time=1.6844 expected time=148.2252
Epoch 49, train_loss=0.6474, train_acc=0.7979 val_loss=1.4333,  val_acc=0.6013
step=0 loss=0.5860 running loss=0.5284 , time=1.6935 expected time=149.0306
Epoch 50, train_loss=0.5794, train_acc=0.8221 val_loss=1.3620,  val_acc=0.6445
step=0 loss=0.6984 running loss=0.6411 , time=1.6543 expected time=145.5783
Epoch 51, train_loss=0.5488, train_acc=0.8271 val_loss=1.4671,  val_acc=0.6213
step=0 loss=0.3373 running loss=0.2805 , time=1.9370 expected time=170.4566
Epoch 52, train_loss=0.5126, train_acc=0.8471 val_loss=1.3480,  val_acc=0.6179
step=0 loss=0.6622 running loss=0.6058 , time=1.6811 expected time=147.9341
Epoch 53, train_loss=0.4660, train_acc=0.8650 val_loss=1.5008,  val_acc=0.6346
step=0 loss=0.1454 running loss=0.0893 , time=1.6813 expected time=147.9533
Epoch 54, train_loss=0.4370, train_acc=0.8593 val_loss=1.3940,  val_acc=0.6478
step=0 loss=0.3194 running loss=0.2638 , time=1.6831 expected time=148.1103
Epoch 55, train_loss=0.4166, train_acc=0.8714 val_loss=1.3501,  val_acc=0.6478
step=0 loss=0.3823 running loss=0.3270 , time=1.7081 expected time=150.3116
Epoch 56, train_loss=0.3998, train_acc=0.8807 val_loss=1.3749,  val_acc=0.6512
step=0 loss=0.3961 running loss=0.3412 , time=1.6812 expected time=147.9435
Epoch 57, train_loss=0.3580, train_acc=0.8979 val_loss=1.4774,  val_acc=0.6346
step=0 loss=0.4257 running loss=0.3710 , time=1.6720 expected time=147.1391
Epoch 58, train_loss=0.3440, train_acc=0.9043 val_loss=1.3594,  val_acc=0.6645
step=0 loss=0.2236 running loss=0.1692 , time=1.6846 expected time=148.2475
Epoch 59, train_loss=0.3073, train_acc=0.9150 val_loss=1.3796,  val_acc=0.6777
step=0 loss=0.3801 running loss=0.3259 , time=1.6927 expected time=148.9591
Epoch 60, train_loss=0.2976, train_acc=0.9157 val_loss=1.3373,  val_acc=0.6777
step=0 loss=0.1720 running loss=0.1182 , time=1.7075 expected time=150.2600
Epoch 61, train_loss=0.2755, train_acc=0.9257 val_loss=1.4110,  val_acc=0.6645
step=0 loss=0.1723 running loss=0.1187 , time=1.9773 expected time=173.9986
Epoch 62, train_loss=0.2723, train_acc=0.9236 val_loss=1.3824,  val_acc=0.6811
step=0 loss=0.1430 running loss=0.0896 , time=1.7039 expected time=149.9442
Epoch 63, train_loss=0.2505, train_acc=0.9350 val_loss=1.4496,  val_acc=0.6711
step=0 loss=0.1849 running loss=0.1316 , time=1.6911 expected time=148.8160
Epoch 64, train_loss=0.2258, train_acc=0.9429 val_loss=1.4490,  val_acc=0.6944
step=0 loss=0.2887 running loss=0.2356 , time=1.7075 expected time=150.2632
Epoch 65, train_loss=0.2089, train_acc=0.9521 val_loss=1.4438,  val_acc=0.6711
step=0 loss=0.2948 running loss=0.2419 , time=1.6815 expected time=147.9760
Epoch 66, train_loss=0.2014, train_acc=0.9543 val_loss=1.4046,  val_acc=0.6811
step=0 loss=0.4537 running loss=0.4009 , time=1.7093 expected time=150.4202
Epoch 67, train_loss=0.1941, train_acc=0.9521 val_loss=1.4401,  val_acc=0.6678
step=0 loss=0.3052 running loss=0.2525 , time=1.6930 expected time=148.9856
Epoch 68, train_loss=0.1939, train_acc=0.9514 val_loss=1.4422,  val_acc=0.6944
step=0 loss=0.2186 running loss=0.1660 , time=1.8219 expected time=160.3264
Epoch 69, train_loss=0.1847, train_acc=0.9607 val_loss=1.4364,  val_acc=0.6977
step=0 loss=0.1425 running loss=0.0899 , time=1.8096 expected time=159.2463
Epoch 70, train_loss=0.1816, train_acc=0.9650 val_loss=1.4534,  val_acc=0.6877
step=0 loss=0.2430 running loss=0.1905 , time=1.8062 expected time=158.9434
Epoch 71, train_loss=0.1806, train_acc=0.9586 val_loss=1.4226,  val_acc=0.6811
step=0 loss=0.1446 running loss=0.0922 , time=2.1256 expected time=187.0527
Epoch 72, train_loss=0.1684, train_acc=0.9643 val_loss=1.4389,  val_acc=0.6744
step=0 loss=0.1839 running loss=0.1314 , time=2.7871 expected time=245.2690
Epoch 73, train_loss=0.1629, train_acc=0.9671 val_loss=1.4605,  val_acc=0.6711
step=0 loss=0.1981 running loss=0.1456 , time=1.7566 expected time=154.5817
Epoch 74, train_loss=0.1784, train_acc=0.9571 val_loss=1.4790,  val_acc=0.6777
step=0 loss=0.1354 running loss=0.0830 , time=2.0881 expected time=183.7547
Epoch 75, train_loss=0.1771, train_acc=0.9593 val_loss=1.4403,  val_acc=0.6777
Evaluate on test set
