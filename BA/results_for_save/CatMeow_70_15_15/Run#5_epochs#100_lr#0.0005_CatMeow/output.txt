Echoing to file start
Save config dict to file
Set seed to 1234
Start loading data
End loading data
Generate Datalaoder
Generate Datalaoder end
Start training
step=0 loss=1.1799 running loss=1.1753 , time=1.3691 expected time=27.3824
Epoch 1, train_loss=1.1943, train_acc=0.2110 val_loss=1.0954,  val_acc=0.2687
step=0 loss=1.0943 running loss=1.0898 , time=0.4297 expected time=8.5931
Epoch 2, train_loss=1.0547, train_acc=0.4903 val_loss=1.1385,  val_acc=0.4776
step=0 loss=1.2045 running loss=1.2002 , time=0.4283 expected time=8.5668
Epoch 3, train_loss=1.0262, train_acc=0.5000 val_loss=1.0747,  val_acc=0.4776
step=0 loss=0.9432 running loss=0.9391 , time=0.4273 expected time=8.5469
Epoch 4, train_loss=0.9907, train_acc=0.5000 val_loss=1.6422,  val_acc=0.4776
step=0 loss=1.0546 running loss=1.0510 , time=0.4436 expected time=8.8727
Epoch 5, train_loss=0.9393, train_acc=0.5097 val_loss=5.4031,  val_acc=0.4776
step=0 loss=1.0023 running loss=0.9999 , time=0.4351 expected time=8.7013
Epoch 6, train_loss=0.9031, train_acc=0.5487 val_loss=3.6936,  val_acc=0.4776
step=0 loss=0.7928 running loss=0.7914 , time=0.4368 expected time=8.7350
Epoch 7, train_loss=0.9190, train_acc=0.5682 val_loss=1.6350,  val_acc=0.3284
step=0 loss=0.8581 running loss=0.8574 , time=0.4401 expected time=8.8011
Epoch 8, train_loss=0.9043, train_acc=0.5357 val_loss=5.6892,  val_acc=0.4776
step=0 loss=0.7907 running loss=0.7904 , time=0.4369 expected time=8.7372
Epoch 9, train_loss=0.8591, train_acc=0.5714 val_loss=1.2077,  val_acc=0.3881
step=0 loss=0.7700 running loss=0.7699 , time=0.4525 expected time=9.0507
Epoch 10, train_loss=0.8710, train_acc=0.6006 val_loss=1.4024,  val_acc=0.2836
step=0 loss=1.0008 running loss=1.0006 , time=0.4382 expected time=8.7643
Epoch 11, train_loss=0.8400, train_acc=0.5519 val_loss=1.4785,  val_acc=0.3433
step=0 loss=0.8165 running loss=0.8160 , time=0.4285 expected time=8.5702
Epoch 12, train_loss=0.8263, train_acc=0.6006 val_loss=0.9679,  val_acc=0.5821
step=0 loss=1.1764 running loss=1.1760 , time=0.4285 expected time=8.5707
Epoch 13, train_loss=0.8941, train_acc=0.5909 val_loss=8.0605,  val_acc=0.4776
step=0 loss=0.7395 running loss=0.7389 , time=0.4304 expected time=8.6078
Epoch 14, train_loss=0.7647, train_acc=0.5942 val_loss=0.8898,  val_acc=0.5373
step=0 loss=0.5608 running loss=0.5603 , time=0.4287 expected time=8.5744
Epoch 15, train_loss=0.7263, train_acc=0.6656 val_loss=1.9637,  val_acc=0.3134
step=0 loss=0.9044 running loss=0.9035 , time=0.4356 expected time=8.7119
Epoch 16, train_loss=0.7472, train_acc=0.5974 val_loss=12.5224,  val_acc=0.4776
step=0 loss=0.6782 running loss=0.6772 , time=0.4290 expected time=8.5799
Epoch 17, train_loss=0.6922, train_acc=0.6623 val_loss=1.0741,  val_acc=0.4478
step=0 loss=0.5188 running loss=0.5184 , time=0.4347 expected time=8.6934
Epoch 18, train_loss=0.8036, train_acc=0.6396 val_loss=1.8962,  val_acc=0.4627
step=0 loss=0.8095 running loss=0.8088 , time=0.4365 expected time=8.7310
Epoch 19, train_loss=0.6942, train_acc=0.6753 val_loss=1.0330,  val_acc=0.5672
step=0 loss=1.5020 running loss=1.5019 , time=0.4262 expected time=8.5242
Epoch 20, train_loss=0.7959, train_acc=0.6364 val_loss=1.3951,  val_acc=0.4478
step=0 loss=0.5164 running loss=0.5160 , time=0.4432 expected time=8.8638
Epoch 21, train_loss=0.7041, train_acc=0.6591 val_loss=1.0112,  val_acc=0.4925
step=0 loss=0.7173 running loss=0.7167 , time=0.4407 expected time=8.8146
Epoch 22, train_loss=0.6696, train_acc=0.6558 val_loss=14.5460,  val_acc=0.4776
step=0 loss=0.4845 running loss=0.4839 , time=0.4420 expected time=8.8408
Epoch 23, train_loss=0.6248, train_acc=0.6851 val_loss=2.3826,  val_acc=0.4776
step=0 loss=0.6067 running loss=0.6058 , time=0.4299 expected time=8.5972
Epoch 24, train_loss=0.6373, train_acc=0.6786 val_loss=2.1157,  val_acc=0.4776
step=0 loss=0.5902 running loss=0.5895 , time=0.4287 expected time=8.5731
Epoch 25, train_loss=0.6486, train_acc=0.6851 val_loss=1.7252,  val_acc=0.4030
step=0 loss=0.5711 running loss=0.5706 , time=0.4280 expected time=8.5591
Epoch 26, train_loss=0.6185, train_acc=0.6981 val_loss=4.1944,  val_acc=0.4776
step=0 loss=0.4765 running loss=0.4764 , time=0.4291 expected time=8.5828
Epoch 27, train_loss=0.6490, train_acc=0.7110 val_loss=1.1861,  val_acc=0.5075
step=0 loss=0.4086 running loss=0.4082 , time=0.4291 expected time=8.5817
Epoch 28, train_loss=0.6411, train_acc=0.6883 val_loss=1.3714,  val_acc=0.5672
step=0 loss=0.7732 running loss=0.7730 , time=0.4337 expected time=8.6742
Epoch 29, train_loss=0.6654, train_acc=0.6721 val_loss=4.5336,  val_acc=0.4776
step=0 loss=0.5161 running loss=0.5158 , time=0.4303 expected time=8.6068
Epoch 30, train_loss=0.6411, train_acc=0.7143 val_loss=3.2789,  val_acc=0.4627
step=0 loss=0.5580 running loss=0.5579 , time=0.4373 expected time=8.7470
Epoch 31, train_loss=0.6367, train_acc=0.6948 val_loss=1.0674,  val_acc=0.5075
step=0 loss=0.6805 running loss=0.6802 , time=0.4282 expected time=8.5638
Epoch 32, train_loss=0.6177, train_acc=0.7078 val_loss=0.8567,  val_acc=0.5821
step=0 loss=0.4062 running loss=0.4057 , time=0.4296 expected time=8.5927
Epoch 33, train_loss=0.7066, train_acc=0.6916 val_loss=1.1095,  val_acc=0.5075
step=0 loss=0.5035 running loss=0.5032 , time=0.4279 expected time=8.5575
Epoch 34, train_loss=0.6563, train_acc=0.6818 val_loss=1.0018,  val_acc=0.4478
step=0 loss=0.4975 running loss=0.4971 , time=0.4322 expected time=8.6433
Epoch 35, train_loss=0.5837, train_acc=0.7565 val_loss=1.3856,  val_acc=0.4030
step=0 loss=0.5328 running loss=0.5325 , time=0.4317 expected time=8.6337
Epoch 36, train_loss=0.6108, train_acc=0.7175 val_loss=4.5203,  val_acc=0.4776
step=0 loss=0.4180 running loss=0.4172 , time=0.4258 expected time=8.5155
Epoch 37, train_loss=0.5979, train_acc=0.7110 val_loss=1.6096,  val_acc=0.4179
step=0 loss=0.4988 running loss=0.4985 , time=0.4275 expected time=8.5501
Epoch 38, train_loss=0.5956, train_acc=0.7078 val_loss=1.2872,  val_acc=0.5373
step=0 loss=1.2259 running loss=1.2255 , time=0.4296 expected time=8.5921
Epoch 39, train_loss=0.6248, train_acc=0.7110 val_loss=24.9155,  val_acc=0.4776
step=0 loss=0.4153 running loss=0.4152 , time=0.4275 expected time=8.5498
Epoch 40, train_loss=0.5312, train_acc=0.7273 val_loss=1.2443,  val_acc=0.4925
step=0 loss=0.4837 running loss=0.4834 , time=0.4299 expected time=8.5981
Epoch 41, train_loss=0.5595, train_acc=0.7435 val_loss=7.0513,  val_acc=0.4776
step=0 loss=0.2609 running loss=0.2607 , time=0.4275 expected time=8.5490
Epoch 42, train_loss=0.6011, train_acc=0.7500 val_loss=1.2509,  val_acc=0.4627
step=0 loss=0.4535 running loss=0.4532 , time=0.4296 expected time=8.5922
Epoch 43, train_loss=0.5794, train_acc=0.7305 val_loss=1.1087,  val_acc=0.4776
step=0 loss=0.5211 running loss=0.5208 , time=0.4284 expected time=8.5675
Epoch 44, train_loss=0.6295, train_acc=0.7305 val_loss=1.8408,  val_acc=0.4627
step=0 loss=0.3636 running loss=0.3634 , time=0.4382 expected time=8.7636
Epoch 45, train_loss=0.5449, train_acc=0.7565 val_loss=1.6259,  val_acc=0.4179
step=0 loss=0.5274 running loss=0.5273 , time=0.4293 expected time=8.5864
Epoch 46, train_loss=0.5211, train_acc=0.7597 val_loss=11.4008,  val_acc=0.4776
step=0 loss=0.5408 running loss=0.5406 , time=0.4383 expected time=8.7665
Epoch 47, train_loss=0.4906, train_acc=0.7468 val_loss=7.2006,  val_acc=0.4776
step=0 loss=0.3554 running loss=0.3552 , time=0.4289 expected time=8.5783
Epoch 48, train_loss=0.5167, train_acc=0.7760 val_loss=13.8042,  val_acc=0.4776
step=0 loss=0.6088 running loss=0.6086 , time=0.4282 expected time=8.5635
Epoch 49, train_loss=0.5343, train_acc=0.7468 val_loss=2.6652,  val_acc=0.5075
step=0 loss=0.6459 running loss=0.6456 , time=0.4295 expected time=8.5906
Epoch 50, train_loss=0.5515, train_acc=0.7175 val_loss=0.9978,  val_acc=0.5224
step=0 loss=0.4272 running loss=0.4268 , time=0.4291 expected time=8.5813
Epoch 51, train_loss=0.5336, train_acc=0.7760 val_loss=3.5110,  val_acc=0.3582
step=0 loss=0.2803 running loss=0.2800 , time=0.4273 expected time=8.5455
Epoch 52, train_loss=0.5289, train_acc=0.7597 val_loss=8.1028,  val_acc=0.4776
step=0 loss=0.6197 running loss=0.6194 , time=0.4296 expected time=8.5921
Epoch 53, train_loss=0.4322, train_acc=0.7922 val_loss=0.9273,  val_acc=0.6119
step=0 loss=0.4246 running loss=0.4245 , time=0.4566 expected time=9.1320
Epoch 54, train_loss=0.4631, train_acc=0.7922 val_loss=1.4975,  val_acc=0.5075
step=0 loss=0.3711 running loss=0.3709 , time=0.4316 expected time=8.6323
Epoch 55, train_loss=0.4922, train_acc=0.7922 val_loss=11.0479,  val_acc=0.4776
step=0 loss=0.4494 running loss=0.4492 , time=0.4282 expected time=8.5643
Epoch 56, train_loss=0.4383, train_acc=0.8052 val_loss=3.0640,  val_acc=0.5224
step=0 loss=0.3111 running loss=0.3107 , time=0.4284 expected time=8.5679
Epoch 57, train_loss=0.4922, train_acc=0.7922 val_loss=1.1203,  val_acc=0.5672
step=0 loss=0.5240 running loss=0.5238 , time=0.4456 expected time=8.9128
Epoch 58, train_loss=0.5177, train_acc=0.7792 val_loss=4.8577,  val_acc=0.3881
step=0 loss=0.3578 running loss=0.3576 , time=0.4332 expected time=8.6635
Epoch 59, train_loss=0.6168, train_acc=0.7403 val_loss=1.6944,  val_acc=0.5373
step=0 loss=0.4861 running loss=0.4852 , time=0.4271 expected time=8.5429
Epoch 60, train_loss=0.6128, train_acc=0.7468 val_loss=1.2440,  val_acc=0.5075
step=0 loss=0.5805 running loss=0.5795 , time=0.4321 expected time=8.6420
Epoch 61, train_loss=0.4544, train_acc=0.7727 val_loss=1.3204,  val_acc=0.5821
step=0 loss=0.4047 running loss=0.4043 , time=0.4293 expected time=8.5862
Epoch 62, train_loss=0.4250, train_acc=0.8214 val_loss=1.1088,  val_acc=0.6269
step=0 loss=0.4012 running loss=0.4009 , time=0.4283 expected time=8.5656
Epoch 63, train_loss=0.4063, train_acc=0.8312 val_loss=1.0253,  val_acc=0.6119
step=0 loss=0.2947 running loss=0.2946 , time=0.4277 expected time=8.5548
Epoch 64, train_loss=0.3873, train_acc=0.8052 val_loss=11.0297,  val_acc=0.4776
step=0 loss=0.3228 running loss=0.3226 , time=0.4292 expected time=8.5839
Epoch 65, train_loss=0.3839, train_acc=0.8019 val_loss=4.2350,  val_acc=0.4776
step=0 loss=0.3064 running loss=0.3062 , time=0.4301 expected time=8.6018
Epoch 66, train_loss=0.4098, train_acc=0.8149 val_loss=2.0086,  val_acc=0.5224
step=0 loss=0.2578 running loss=0.2577 , time=0.4292 expected time=8.5837
Epoch 67, train_loss=0.4000, train_acc=0.8442 val_loss=0.9709,  val_acc=0.6269
step=0 loss=0.5058 running loss=0.5056 , time=0.4329 expected time=8.6588
Epoch 68, train_loss=0.3465, train_acc=0.8539 val_loss=0.9364,  val_acc=0.6119
step=0 loss=0.4208 running loss=0.4207 , time=0.4286 expected time=8.5716
Epoch 69, train_loss=0.3547, train_acc=0.8571 val_loss=1.6418,  val_acc=0.5672
step=0 loss=0.1940 running loss=0.1940 , time=0.4271 expected time=8.5428
Epoch 70, train_loss=0.3396, train_acc=0.8636 val_loss=0.9908,  val_acc=0.6716
step=0 loss=0.3071 running loss=0.3069 , time=0.4282 expected time=8.5649
Epoch 71, train_loss=0.3373, train_acc=0.8636 val_loss=7.3271,  val_acc=0.4776
step=0 loss=0.5234 running loss=0.5233 , time=0.4306 expected time=8.6116
Epoch 72, train_loss=0.3192, train_acc=0.8701 val_loss=1.0323,  val_acc=0.6567
step=0 loss=0.4298 running loss=0.4297 , time=0.4556 expected time=9.1118
Epoch 73, train_loss=0.3130, train_acc=0.8734 val_loss=2.9169,  val_acc=0.4925
step=0 loss=0.2079 running loss=0.2078 , time=0.4293 expected time=8.5853
Epoch 74, train_loss=0.3094, train_acc=0.8636 val_loss=1.0952,  val_acc=0.6418
step=0 loss=0.2150 running loss=0.2149 , time=0.4342 expected time=8.6832
Epoch 75, train_loss=0.2993, train_acc=0.8734 val_loss=1.0464,  val_acc=0.6866
step=0 loss=0.3109 running loss=0.3107 , time=0.4313 expected time=8.6267
Epoch 76, train_loss=0.3033, train_acc=0.8864 val_loss=2.9701,  val_acc=0.5075
step=0 loss=0.2759 running loss=0.2758 , time=0.4292 expected time=8.5848
Epoch 77, train_loss=0.2975, train_acc=0.8896 val_loss=1.1462,  val_acc=0.5373
step=0 loss=0.6533 running loss=0.6533 , time=0.4322 expected time=8.6436
Epoch 78, train_loss=0.2717, train_acc=0.8961 val_loss=1.1194,  val_acc=0.6418
step=0 loss=0.0978 running loss=0.0977 , time=0.4314 expected time=8.6278
Epoch 79, train_loss=0.2773, train_acc=0.9091 val_loss=1.6069,  val_acc=0.6119
step=0 loss=0.2429 running loss=0.2428 , time=0.4297 expected time=8.5946
Epoch 80, train_loss=0.2971, train_acc=0.8994 val_loss=1.1799,  val_acc=0.5373
step=0 loss=0.5269 running loss=0.5267 , time=0.4699 expected time=9.3976
Epoch 81, train_loss=0.2949, train_acc=0.8799 val_loss=1.1847,  val_acc=0.5821
step=0 loss=0.3164 running loss=0.3163 , time=0.4309 expected time=8.6188
Epoch 82, train_loss=0.2892, train_acc=0.8929 val_loss=1.0470,  val_acc=0.6866
step=0 loss=0.1893 running loss=0.1891 , time=0.4316 expected time=8.6317
Epoch 83, train_loss=0.2570, train_acc=0.9091 val_loss=1.1580,  val_acc=0.6418
step=0 loss=0.1593 running loss=0.1592 , time=0.4313 expected time=8.6257
Epoch 84, train_loss=0.2546, train_acc=0.8994 val_loss=2.4777,  val_acc=0.5224
step=0 loss=0.1670 running loss=0.1670 , time=0.4305 expected time=8.6098
Epoch 85, train_loss=0.2551, train_acc=0.9091 val_loss=1.2881,  val_acc=0.6567
step=0 loss=0.2735 running loss=0.2735 , time=0.4301 expected time=8.6020
Epoch 86, train_loss=0.2658, train_acc=0.9026 val_loss=1.1713,  val_acc=0.6269
step=0 loss=0.4827 running loss=0.4827 , time=0.4297 expected time=8.5932
Epoch 87, train_loss=0.2247, train_acc=0.9123 val_loss=1.1568,  val_acc=0.6269
step=0 loss=0.4758 running loss=0.4758 , time=0.4511 expected time=9.0223
Epoch 88, train_loss=0.2696, train_acc=0.8994 val_loss=1.1469,  val_acc=0.6119
step=0 loss=0.2820 running loss=0.2820 , time=0.4290 expected time=8.5803
Epoch 89, train_loss=0.2115, train_acc=0.9318 val_loss=1.2294,  val_acc=0.5522
step=0 loss=0.3013 running loss=0.3012 , time=0.4319 expected time=8.6388
Epoch 90, train_loss=0.2229, train_acc=0.9123 val_loss=1.1214,  val_acc=0.5970
step=0 loss=0.4948 running loss=0.4948 , time=0.4348 expected time=8.6965
Epoch 91, train_loss=0.2509, train_acc=0.9091 val_loss=1.1407,  val_acc=0.6418
step=0 loss=0.4375 running loss=0.4375 , time=0.4467 expected time=8.9331
Epoch 92, train_loss=0.2456, train_acc=0.8994 val_loss=1.1403,  val_acc=0.5672
step=0 loss=0.1782 running loss=0.1782 , time=0.4390 expected time=8.7808
Epoch 93, train_loss=0.2447, train_acc=0.9383 val_loss=1.1535,  val_acc=0.6119
step=0 loss=0.1172 running loss=0.1172 , time=0.4303 expected time=8.6070
Epoch 94, train_loss=0.2688, train_acc=0.9026 val_loss=1.1299,  val_acc=0.5821
step=0 loss=0.4904 running loss=0.4904 , time=0.4304 expected time=8.6081
Epoch 95, train_loss=0.2268, train_acc=0.9188 val_loss=1.1217,  val_acc=0.5821
step=0 loss=0.5041 running loss=0.5041 , time=0.4393 expected time=8.7863
Epoch 96, train_loss=0.2304, train_acc=0.9221 val_loss=1.2198,  val_acc=0.6119
step=0 loss=0.4014 running loss=0.4014 , time=0.4281 expected time=8.5629
Epoch 97, train_loss=0.2251, train_acc=0.9221 val_loss=1.2057,  val_acc=0.5821
step=0 loss=0.3251 running loss=0.3251 , time=0.4467 expected time=8.9333
Epoch 98, train_loss=0.1911, train_acc=0.9416 val_loss=1.1788,  val_acc=0.5821
step=0 loss=0.1666 running loss=0.1666 , time=0.4285 expected time=8.5695
Epoch 99, train_loss=0.2125, train_acc=0.9221 val_loss=1.1714,  val_acc=0.5821
step=0 loss=0.2678 running loss=0.2678 , time=0.4299 expected time=8.5975
Epoch 100, train_loss=0.2909, train_acc=0.9026 val_loss=1.1498,  val_acc=0.6119
Evaluate on test set
