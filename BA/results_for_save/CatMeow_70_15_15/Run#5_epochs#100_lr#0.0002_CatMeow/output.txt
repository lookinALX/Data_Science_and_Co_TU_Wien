Echoing to file start
Save config dict to file
Set seed to 1234
Start loading data
End loading data
Generate Datalaoder
Generate Datalaoder end
Start training
step=0 loss=1.1799 running loss=1.1753 , time=1.3351 expected time=26.7015
Epoch 1, train_loss=1.2270, train_acc=0.2013 val_loss=1.1366,  val_acc=0.2537
step=0 loss=1.1774 running loss=1.1729 , time=0.4338 expected time=8.6750
Epoch 2, train_loss=1.1104, train_acc=0.2955 val_loss=1.0852,  val_acc=0.4776
step=0 loss=1.1151 running loss=1.1107 , time=0.4280 expected time=8.5608
Epoch 3, train_loss=1.0263, train_acc=0.5000 val_loss=1.1285,  val_acc=0.4776
step=0 loss=0.9572 running loss=0.9529 , time=0.4413 expected time=8.8263
Epoch 4, train_loss=1.0170, train_acc=0.5000 val_loss=1.0886,  val_acc=0.4776
step=0 loss=1.0638 running loss=1.0597 , time=0.4313 expected time=8.6255
Epoch 5, train_loss=1.0026, train_acc=0.5000 val_loss=1.0771,  val_acc=0.4776
step=0 loss=1.0448 running loss=1.0410 , time=0.4862 expected time=9.7249
Epoch 6, train_loss=0.9846, train_acc=0.5000 val_loss=1.0252,  val_acc=0.4627
step=0 loss=0.9014 running loss=0.8980 , time=0.4281 expected time=8.5614
Epoch 7, train_loss=0.9502, train_acc=0.5195 val_loss=2.8161,  val_acc=0.4776
step=0 loss=0.9543 running loss=0.9516 , time=0.4275 expected time=8.5508
Epoch 8, train_loss=0.9242, train_acc=0.5357 val_loss=2.4273,  val_acc=0.4776
step=0 loss=0.7707 running loss=0.7688 , time=0.4297 expected time=8.5938
Epoch 9, train_loss=0.8756, train_acc=0.5779 val_loss=1.3291,  val_acc=0.3582
step=0 loss=0.8313 running loss=0.8302 , time=0.4293 expected time=8.5862
Epoch 10, train_loss=0.8861, train_acc=0.5682 val_loss=4.0466,  val_acc=0.4776
step=0 loss=1.0536 running loss=1.0530 , time=0.4266 expected time=8.5321
Epoch 11, train_loss=0.8541, train_acc=0.5519 val_loss=2.5461,  val_acc=0.3134
step=0 loss=0.7771 running loss=0.7769 , time=0.4300 expected time=8.5992
Epoch 12, train_loss=0.8817, train_acc=0.5877 val_loss=4.1674,  val_acc=0.4776
step=0 loss=0.9997 running loss=0.9995 , time=0.4305 expected time=8.6099
Epoch 13, train_loss=0.8924, train_acc=0.5519 val_loss=3.6299,  val_acc=0.4776
step=0 loss=0.7662 running loss=0.7660 , time=0.4291 expected time=8.5830
Epoch 14, train_loss=0.8120, train_acc=0.5877 val_loss=0.9420,  val_acc=0.5224
step=0 loss=0.7273 running loss=0.7272 , time=0.4285 expected time=8.5708
Epoch 15, train_loss=0.7837, train_acc=0.6461 val_loss=1.8887,  val_acc=0.3284
step=0 loss=1.0165 running loss=1.0161 , time=0.4316 expected time=8.6316
Epoch 16, train_loss=0.8206, train_acc=0.5617 val_loss=1.4619,  val_acc=0.4627
step=0 loss=0.5964 running loss=0.5961 , time=0.4291 expected time=8.5827
Epoch 17, train_loss=0.7587, train_acc=0.6364 val_loss=2.0670,  val_acc=0.3284
step=0 loss=0.6029 running loss=0.6027 , time=0.4296 expected time=8.5917
Epoch 18, train_loss=0.8269, train_acc=0.6071 val_loss=11.3639,  val_acc=0.4776
step=0 loss=0.9323 running loss=0.9321 , time=0.4317 expected time=8.6346
Epoch 19, train_loss=0.7655, train_acc=0.6104 val_loss=1.8825,  val_acc=0.3284
step=0 loss=1.3053 running loss=1.3050 , time=0.4333 expected time=8.6668
Epoch 20, train_loss=0.7925, train_acc=0.6104 val_loss=2.3247,  val_acc=0.4776
step=0 loss=0.6318 running loss=0.6316 , time=0.4305 expected time=8.6094
Epoch 21, train_loss=0.7647, train_acc=0.6071 val_loss=6.5806,  val_acc=0.4776
step=0 loss=0.6798 running loss=0.6797 , time=0.4312 expected time=8.6234
Epoch 22, train_loss=0.7083, train_acc=0.6331 val_loss=9.0468,  val_acc=0.4776
step=0 loss=0.6291 running loss=0.6291 , time=0.4313 expected time=8.6254
Epoch 23, train_loss=0.7309, train_acc=0.6331 val_loss=1.2130,  val_acc=0.4478
step=0 loss=0.6300 running loss=0.6298 , time=0.4295 expected time=8.5893
Epoch 24, train_loss=0.7229, train_acc=0.6299 val_loss=1.7943,  val_acc=0.3284
step=0 loss=0.6841 running loss=0.6840 , time=0.4305 expected time=8.6108
Epoch 25, train_loss=0.6605, train_acc=0.6558 val_loss=1.1192,  val_acc=0.4925
step=0 loss=0.6273 running loss=0.6269 , time=0.4289 expected time=8.5784
Epoch 26, train_loss=0.6905, train_acc=0.6558 val_loss=0.9522,  val_acc=0.5522
step=0 loss=0.5329 running loss=0.5327 , time=0.4306 expected time=8.6123
Epoch 27, train_loss=0.7377, train_acc=0.6461 val_loss=3.2960,  val_acc=0.2985
step=0 loss=0.6037 running loss=0.6036 , time=0.4391 expected time=8.7826
Epoch 28, train_loss=0.6816, train_acc=0.6753 val_loss=1.2777,  val_acc=0.4179
step=0 loss=0.7379 running loss=0.7378 , time=0.4324 expected time=8.6485
Epoch 29, train_loss=0.7036, train_acc=0.6656 val_loss=10.2502,  val_acc=0.4776
step=0 loss=0.5671 running loss=0.5668 , time=0.4312 expected time=8.6250
Epoch 30, train_loss=0.7039, train_acc=0.6331 val_loss=8.1043,  val_acc=0.4776
step=0 loss=0.5758 running loss=0.5756 , time=0.4335 expected time=8.6707
Epoch 31, train_loss=0.7026, train_acc=0.6721 val_loss=1.0748,  val_acc=0.4627
step=0 loss=0.8647 running loss=0.8645 , time=0.4309 expected time=8.6171
Epoch 32, train_loss=0.6941, train_acc=0.6623 val_loss=9.7148,  val_acc=0.4776
step=0 loss=0.6371 running loss=0.6371 , time=0.4310 expected time=8.6193
Epoch 33, train_loss=0.6670, train_acc=0.6786 val_loss=1.6021,  val_acc=0.4627
step=0 loss=0.5099 running loss=0.5097 , time=0.4324 expected time=8.6487
Epoch 34, train_loss=0.6338, train_acc=0.6883 val_loss=1.4178,  val_acc=0.4478
step=0 loss=0.5365 running loss=0.5362 , time=0.4343 expected time=8.6860
Epoch 35, train_loss=0.5841, train_acc=0.6981 val_loss=1.3578,  val_acc=0.4328
step=0 loss=0.5237 running loss=0.5236 , time=0.4310 expected time=8.6202
Epoch 36, train_loss=0.6444, train_acc=0.7143 val_loss=1.5105,  val_acc=0.4328
step=0 loss=0.4654 running loss=0.4652 , time=0.4522 expected time=9.0446
Epoch 37, train_loss=0.6635, train_acc=0.6883 val_loss=1.4919,  val_acc=0.3881
step=0 loss=0.5133 running loss=0.5130 , time=0.4470 expected time=8.9400
Epoch 38, train_loss=0.6406, train_acc=0.7078 val_loss=0.8876,  val_acc=0.5373
step=0 loss=1.3533 running loss=1.3531 , time=0.4370 expected time=8.7409
Epoch 39, train_loss=0.6465, train_acc=0.7013 val_loss=3.1873,  val_acc=0.4776
step=0 loss=0.4363 running loss=0.4362 , time=0.4583 expected time=9.1667
Epoch 40, train_loss=0.6048, train_acc=0.7143 val_loss=1.5197,  val_acc=0.4328
step=0 loss=0.6736 running loss=0.6734 , time=0.4588 expected time=9.1766
Epoch 41, train_loss=0.6216, train_acc=0.6916 val_loss=11.2339,  val_acc=0.4776
step=0 loss=0.3968 running loss=0.3967 , time=0.4317 expected time=8.6342
Epoch 42, train_loss=0.7548, train_acc=0.7143 val_loss=1.1505,  val_acc=0.4776
step=0 loss=0.4953 running loss=0.4952 , time=0.4326 expected time=8.6515
Epoch 43, train_loss=0.6011, train_acc=0.6851 val_loss=2.3535,  val_acc=0.3582
step=0 loss=0.4659 running loss=0.4655 , time=0.4395 expected time=8.7897
Epoch 44, train_loss=0.6331, train_acc=0.7045 val_loss=1.0763,  val_acc=0.4925
step=0 loss=0.4176 running loss=0.4174 , time=0.4319 expected time=8.6384
Epoch 45, train_loss=0.5649, train_acc=0.7273 val_loss=1.1811,  val_acc=0.4776
step=0 loss=0.4723 running loss=0.4721 , time=0.4834 expected time=9.6673
Epoch 46, train_loss=0.5529, train_acc=0.7500 val_loss=10.6814,  val_acc=0.4776
step=0 loss=0.7032 running loss=0.7030 , time=0.4388 expected time=8.7769
Epoch 47, train_loss=0.5331, train_acc=0.7273 val_loss=0.9506,  val_acc=0.5672
step=0 loss=0.5103 running loss=0.5100 , time=0.4407 expected time=8.8146
Epoch 48, train_loss=0.5825, train_acc=0.7403 val_loss=0.9938,  val_acc=0.6119
step=0 loss=0.7419 running loss=0.7417 , time=0.4678 expected time=9.3567
Epoch 49, train_loss=0.5840, train_acc=0.7045 val_loss=1.1860,  val_acc=0.5672
step=0 loss=0.6440 running loss=0.6439 , time=0.4341 expected time=8.6813
Epoch 50, train_loss=0.5697, train_acc=0.7013 val_loss=1.0477,  val_acc=0.5224
step=0 loss=0.5235 running loss=0.5234 , time=0.4344 expected time=8.6873
Epoch 51, train_loss=0.5364, train_acc=0.7305 val_loss=2.6164,  val_acc=0.4627
step=0 loss=0.2985 running loss=0.2983 , time=0.4348 expected time=8.6957
Epoch 52, train_loss=0.5742, train_acc=0.7273 val_loss=15.0336,  val_acc=0.4776
step=0 loss=0.7212 running loss=0.7210 , time=0.4375 expected time=8.7509
Epoch 53, train_loss=0.4919, train_acc=0.7273 val_loss=12.1292,  val_acc=0.4776
step=0 loss=0.4434 running loss=0.4434 , time=0.4975 expected time=9.9490
Epoch 54, train_loss=0.5097, train_acc=0.7338 val_loss=1.5183,  val_acc=0.4776
step=0 loss=0.4424 running loss=0.4424 , time=0.4392 expected time=8.7841
Epoch 55, train_loss=0.5525, train_acc=0.7370 val_loss=0.9855,  val_acc=0.5224
step=0 loss=0.5774 running loss=0.5774 , time=0.4557 expected time=9.1144
Epoch 56, train_loss=0.5105, train_acc=0.7597 val_loss=1.4237,  val_acc=0.4925
step=0 loss=0.3873 running loss=0.3872 , time=0.4338 expected time=8.6755
Epoch 57, train_loss=0.5064, train_acc=0.7468 val_loss=2.4441,  val_acc=0.3433
step=0 loss=0.7532 running loss=0.7531 , time=0.4449 expected time=8.8973
Epoch 58, train_loss=0.6019, train_acc=0.7468 val_loss=1.0224,  val_acc=0.5373
step=0 loss=0.4886 running loss=0.4886 , time=0.4403 expected time=8.8052
Epoch 59, train_loss=0.5380, train_acc=0.7532 val_loss=2.3058,  val_acc=0.4030
step=0 loss=0.4884 running loss=0.4882 , time=0.4361 expected time=8.7221
Epoch 60, train_loss=0.5425, train_acc=0.7305 val_loss=2.0125,  val_acc=0.4776
step=0 loss=0.7000 running loss=0.6999 , time=0.4455 expected time=8.9102
Epoch 61, train_loss=0.5301, train_acc=0.7370 val_loss=0.9265,  val_acc=0.6119
step=0 loss=0.5603 running loss=0.5603 , time=0.4410 expected time=8.8203
Epoch 62, train_loss=0.5101, train_acc=0.7435 val_loss=1.0669,  val_acc=0.6269
step=0 loss=0.6060 running loss=0.6060 , time=0.4712 expected time=9.4234
Epoch 63, train_loss=0.4751, train_acc=0.7597 val_loss=1.0157,  val_acc=0.6269
step=0 loss=0.3987 running loss=0.3986 , time=0.4552 expected time=9.1049
Epoch 64, train_loss=0.4582, train_acc=0.7662 val_loss=1.3568,  val_acc=0.4776
step=0 loss=0.4742 running loss=0.4742 , time=0.5442 expected time=10.8848
Epoch 65, train_loss=0.4700, train_acc=0.7760 val_loss=1.3336,  val_acc=0.4776
step=0 loss=0.3814 running loss=0.3814 , time=0.4890 expected time=9.7808
Epoch 66, train_loss=0.4868, train_acc=0.7695 val_loss=1.2204,  val_acc=0.5075
step=0 loss=0.2441 running loss=0.2440 , time=0.4815 expected time=9.6293
Epoch 67, train_loss=0.5931, train_acc=0.7727 val_loss=1.6044,  val_acc=0.4627
step=0 loss=0.6938 running loss=0.6937 , time=0.5097 expected time=10.1947
Epoch 68, train_loss=0.4807, train_acc=0.7565 val_loss=1.0036,  val_acc=0.5224
step=0 loss=0.6848 running loss=0.6848 , time=0.4783 expected time=9.5662
Epoch 69, train_loss=0.4893, train_acc=0.7792 val_loss=3.3126,  val_acc=0.4627
step=0 loss=0.3118 running loss=0.3117 , time=0.4652 expected time=9.3039
Epoch 70, train_loss=0.4635, train_acc=0.7695 val_loss=1.0315,  val_acc=0.5970
step=0 loss=0.5336 running loss=0.5336 , time=0.4798 expected time=9.5951
Epoch 71, train_loss=0.4865, train_acc=0.7662 val_loss=1.0793,  val_acc=0.6269
step=0 loss=0.7237 running loss=0.7236 , time=0.4481 expected time=8.9614
Epoch 72, train_loss=0.4551, train_acc=0.7727 val_loss=1.1181,  val_acc=0.5224
step=0 loss=0.4583 running loss=0.4582 , time=0.4488 expected time=8.9760
Epoch 73, train_loss=0.4346, train_acc=0.7857 val_loss=1.4879,  val_acc=0.4776
step=0 loss=0.3075 running loss=0.3074 , time=0.4549 expected time=9.0980
Epoch 74, train_loss=0.4529, train_acc=0.7435 val_loss=1.0141,  val_acc=0.5522
step=0 loss=0.3538 running loss=0.3538 , time=0.4530 expected time=9.0593
Epoch 75, train_loss=0.4534, train_acc=0.7792 val_loss=1.1138,  val_acc=0.5224
step=0 loss=0.3642 running loss=0.3642 , time=0.4667 expected time=9.3341
Epoch 76, train_loss=0.4501, train_acc=0.7857 val_loss=1.7705,  val_acc=0.4776
step=0 loss=0.4256 running loss=0.4255 , time=0.4644 expected time=9.2882
Epoch 77, train_loss=0.4340, train_acc=0.7792 val_loss=1.0792,  val_acc=0.5075
step=0 loss=0.6767 running loss=0.6767 , time=0.4731 expected time=9.4611
Epoch 78, train_loss=0.4297, train_acc=0.8052 val_loss=1.1136,  val_acc=0.5821
step=0 loss=0.1842 running loss=0.1841 , time=0.4663 expected time=9.3252
Epoch 79, train_loss=0.5090, train_acc=0.8019 val_loss=1.1009,  val_acc=0.5821
step=0 loss=0.4515 running loss=0.4515 , time=0.4634 expected time=9.2674
Epoch 80, train_loss=0.4265, train_acc=0.7825 val_loss=1.0768,  val_acc=0.5075
step=0 loss=0.4841 running loss=0.4841 , time=0.4791 expected time=9.5816
Epoch 81, train_loss=0.4371, train_acc=0.7987 val_loss=1.0821,  val_acc=0.5075
step=0 loss=0.4073 running loss=0.4073 , time=0.4425 expected time=8.8497
Epoch 82, train_loss=0.4525, train_acc=0.8019 val_loss=1.0828,  val_acc=0.5821
step=0 loss=0.3898 running loss=0.3898 , time=0.4490 expected time=8.9792
Epoch 83, train_loss=0.4219, train_acc=0.8117 val_loss=0.9912,  val_acc=0.6418
step=0 loss=0.3311 running loss=0.3311 , time=0.4524 expected time=9.0486
Epoch 84, train_loss=0.4231, train_acc=0.8019 val_loss=0.9984,  val_acc=0.6269
step=0 loss=0.3055 running loss=0.3055 , time=0.4574 expected time=9.1477
Epoch 85, train_loss=0.4205, train_acc=0.7825 val_loss=1.0675,  val_acc=0.5224
step=0 loss=0.4397 running loss=0.4397 , time=0.4471 expected time=8.9424
Epoch 86, train_loss=0.4196, train_acc=0.7987 val_loss=1.1103,  val_acc=0.5821
step=0 loss=0.6086 running loss=0.6086 , time=0.4514 expected time=9.0285
Epoch 87, train_loss=0.3991, train_acc=0.8052 val_loss=0.9975,  val_acc=0.6269
step=0 loss=0.5972 running loss=0.5972 , time=0.4853 expected time=9.7057
Epoch 88, train_loss=0.4640, train_acc=0.7662 val_loss=1.1659,  val_acc=0.5672
step=0 loss=0.3746 running loss=0.3746 , time=0.4721 expected time=9.4414
Epoch 89, train_loss=0.4103, train_acc=0.7987 val_loss=1.0338,  val_acc=0.6269
step=0 loss=0.5409 running loss=0.5409 , time=0.4777 expected time=9.5535
Epoch 90, train_loss=0.4067, train_acc=0.8019 val_loss=1.0387,  val_acc=0.6269
step=0 loss=0.5412 running loss=0.5412 , time=0.4817 expected time=9.6345
Epoch 91, train_loss=0.4309, train_acc=0.8019 val_loss=1.0232,  val_acc=0.6269
step=0 loss=0.5288 running loss=0.5288 , time=0.4752 expected time=9.5038
Epoch 92, train_loss=0.4186, train_acc=0.7922 val_loss=0.9859,  val_acc=0.6269
step=0 loss=0.3300 running loss=0.3300 , time=0.4539 expected time=9.0790
Epoch 93, train_loss=0.4088, train_acc=0.8214 val_loss=1.0277,  val_acc=0.6269
step=0 loss=0.2791 running loss=0.2791 , time=0.4475 expected time=8.9497
Epoch 94, train_loss=0.4328, train_acc=0.8084 val_loss=1.0782,  val_acc=0.6269
step=0 loss=0.5322 running loss=0.5321 , time=0.4393 expected time=8.7853
Epoch 95, train_loss=0.4209, train_acc=0.7987 val_loss=1.0513,  val_acc=0.6269
step=0 loss=0.7154 running loss=0.7154 , time=0.4411 expected time=8.8225
Epoch 96, train_loss=0.4655, train_acc=0.8052 val_loss=1.1062,  val_acc=0.5970
step=0 loss=0.6558 running loss=0.6558 , time=0.4397 expected time=8.7931
Epoch 97, train_loss=0.4080, train_acc=0.8084 val_loss=1.0419,  val_acc=0.6269
step=0 loss=0.6188 running loss=0.6188 , time=0.4560 expected time=9.1204
Epoch 98, train_loss=0.3990, train_acc=0.8084 val_loss=1.0270,  val_acc=0.6418
step=0 loss=0.5002 running loss=0.5002 , time=0.4510 expected time=9.0207
Epoch 99, train_loss=0.4199, train_acc=0.7987 val_loss=1.0114,  val_acc=0.6269
step=0 loss=0.4567 running loss=0.4567 , time=0.4595 expected time=9.1894
Epoch 100, train_loss=0.4565, train_acc=0.8182 val_loss=1.0296,  val_acc=0.6269
Evaluate on test set
