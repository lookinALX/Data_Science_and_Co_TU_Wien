{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b1bd4841-a4fd-4ec1-8fc2-3344386f9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "data_folder_path = os.path.join(current_directory, 'data')\n",
    "datasets_path_file = os.path.join(data_folder_path, 'datasets_path.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ffae8a8f-6e4e-4329-a154-0e41d43195e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datasets_path_file, 'r', encoding='utf-8') as file:\n",
    "    datasets_path = json.load(file)\n",
    "\n",
    "path_to_UrbanSound8K = datasets_path.get(\"UrbanSound8K\", None)\n",
    "path_to_Cat = datasets_path.get(\"CatMeow\", None)\n",
    "path_to_ESC = datasets_path.get(\"ESC50\", None)\n",
    "path_to_minds14 = datasets_path.get(\"MINDS14\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a7439-bef4-419e-9e99-f7b202e25d76",
   "metadata": {},
   "source": [
    "### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f81a641-43b3-4d55-8236-9bdd8cdbbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mono_if_stereo(waveform):\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform\n",
    "    else:\n",
    "        return waveform\n",
    "\n",
    "def resample_if_not_sample_rate(waveform, sr):\n",
    "    if sr == self.sample_rate:\n",
    "        return waveform\n",
    "    else:\n",
    "        resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        return waveform\n",
    "\n",
    "def make_fixed_length(waveform):\n",
    "    if waveform.shape[1] < self.num_samples:\n",
    "        # pad with zeros if too short\n",
    "        padding = self.num_samples - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        return waveform\n",
    "    elif waveform.shape[1] > self.num_samples:\n",
    "        # randomly crop if too long\n",
    "        start = np.random.randint(0, waveform.shape[1] - self.num_samples)\n",
    "        waveform = waveform[:, start:start + self.num_samples]\n",
    "        return waveform\n",
    "    else:\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb4d9d-4b44-48d8-8ad0-32343e25c7f3",
   "metadata": {},
   "source": [
    "### Define Datasets Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797256b9-4fcc-4c60-a373-9e7391406502",
   "metadata": {},
   "source": [
    "#### UrbanSound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b148520e-1148-481f-875c-10417974402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, metadata_file, audio_dir, sample_rate=16000, num_samples=160000, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = num_samples \n",
    "        self.transform = transform\n",
    "        # map class labels to integers\n",
    "        self.class_mapping = {label: idx for idx, label in enumerate(sorted(self.metadata['class'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(\n",
    "            self.audio_dir,\n",
    "            f\"fold{self.metadata.iloc[idx]['fold']}\", \n",
    "            self.metadata.iloc[idx]['slice_file_name']\n",
    "        )\n",
    "        #TODO: first load --> foeach get by idx\n",
    "        waveform, sr = torchaudio.load(audio_path) # load waveform and sample rate\n",
    "        waveform = to_mono_if_stereo(waveform)\n",
    "        waveform = resample_if_not_sample_rate(waveform, sr)\n",
    "        waveform = make_fixed_length(waveform)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        label = self.class_mapping[self.metadata.ilococ[idx]['class']]\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f73cc-945c-4261-85fd-a9d4e2fd09f4",
   "metadata": {},
   "source": [
    "#### MINDS-14 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "896cdb11-eabc-4cb1-a55e-77faa7deae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINDS14Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, split='train', sample_rate=16000, num_samples=160000, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.dataset = load_from_disk(dataset_path)\n",
    "        self.data = self.dataset[split]\n",
    "        self.intents = sorted(list(set(self.data['intent_class'])))\n",
    "        self.intent_to_idx = {intent: idx for idx, intent in enumerate(self.intents)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_data = self.data[idx]['audio']\n",
    "        audio_array = np.array(audio_data['array'])\n",
    "        sr = audio_data['sampling_rate']\n",
    "        intent = self.data[idx]['intent_class']\n",
    "        # convert audio to tensor, due to hugging face download\n",
    "        waveform = torch.FloatTensor(audio_array).unsqueeze(0)\n",
    "        waveform = to_mono_if_stereo(waveform)\n",
    "        waveform = resample_if_not_sample_rate(waveform, sr)\n",
    "        waveform = make_fixed_length(waveform)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        label = self.intent_to_idx[intent]\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a68d2-e095-491a-b036-3b4210c9c7db",
   "metadata": {},
   "source": [
    "#### CatMeow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54faa083-e5f3-4f37-8bef-060230533188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatMeowDataset(Dataset):\n",
    "    def __init__(self, metadata_df, audio_dir, sample_rate=16000, num_samples=160000, transform=None):\n",
    "        self.metadata = metadata_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        # map class labels to integers (using emission context as class)\n",
    "        self.class_mapping = {label: idx for idx, label in enumerate(sorted(self.metadata['context'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_filename = self.metadata.iloc[idx]['filename']\n",
    "        audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        \n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        waveform = to_mono_if_stereo(waveform)\n",
    "        waveform = resample_if_not_sample_rate(waveform, sr)\n",
    "        waveform = make_fixed_length(waveform)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        label = self.class_mapping[self.metadata.ilococ[idx]['class']]\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fb54c-fe8c-43e2-81a1-8a94438e662f",
   "metadata": {},
   "source": [
    "#### ESC-50 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80695b15-6524-4d76-8bb4-1236ff7468b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC50Dataset(Dataset):\n",
    "    def __init__(self, metadata_df, audio_dir, sample_rate=16000, num_samples=160000, transform=None):\n",
    "        self.metadata = metadata_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = num_samples \n",
    "        self.transform = transform\n",
    "        # map class labels to integers\n",
    "        self.class_mapping = {label: idx for idx, label in enumerate(sorted(self.metadata['category'].unique()))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_filename = self.metadata.iloc[idx]['filename']\n",
    "        audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        \n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        waveform = to_mono_if_stereo(waveform)\n",
    "        waveform = resample_if_not_sample_rate(waveform, sr)\n",
    "        waveform = make_fixed_length(waveform)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        label = self.class_mapping[self.metadata.iloc[idx]['category']]\n",
    "        \n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffe152-a633-42b3-8b4b-70dc971e5265",
   "metadata": {},
   "source": [
    "### Preparation Functions per Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d475c-03dc-4580-8aa8-a49cd4d920d4",
   "metadata": {},
   "source": [
    "#### UrbanSound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5523c130-b67f-460a-aef8-5b7567ab6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_urbansound_dataset(path_to_UrbanSound8K, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare the UrbanSound8K dataset for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        path_to_UrbanSound8K (str): Path to the UrbanSound8K dataset root directory\n",
    "        batch_size (int): Batch size for data loaders\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, class_mapping\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(path_to_UrbanSound8K, 'metadata', 'UrbanSound8K.csv')\n",
    "    audio_dir = os.path.join(path_to_UrbanSound8K, 'audio')\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    \n",
    "    # Create stratified splits\n",
    "    train_meta, test_meta = train_test_split(\n",
    "        metadata, test_size=0.2, random_state=42, stratify=metadata['class']\n",
    "    )\n",
    "    train_meta, val_meta = train_test_split(\n",
    "        train_meta, test_size=0.25, random_state=42, stratify=train_meta['class']\n",
    "    )  # 0.25 x 0.8 = 0.2 of total\n",
    "    \n",
    "    # Save splits\n",
    "    train_meta.to_csv(os.path.join(path_to_UrbanSound8K, 'metadata', 'train.csv'), index=False)\n",
    "    val_meta.to_csv(os.path.join(path_to_UrbanSound8K, 'metadata', 'val.csv'), index=False)\n",
    "    test_meta.to_csv(os.path.join(path_to_UrbanSound8K, 'metadata', 'test.csv'), index=False)\n",
    "    \n",
    "    # Define data augmentation transformations\n",
    "    train_transform = torch.nn.Sequential(\n",
    "        torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "        torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = UrbanSoundDataset(\n",
    "        os.path.join(path_to_UrbanSound8K, 'metadata', 'train.csv'),\n",
    "        audio_dir,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = UrbanSoundDataset(\n",
    "        os.path.join(path_to_UrbanSound8K, 'metadata', 'val.csv'),\n",
    "        audio_dir\n",
    "    )\n",
    "    \n",
    "    test_dataset = UrbanSoundDataset(\n",
    "        os.path.join(path_to_UrbanSound8K, 'metadata', 'test.csv'),\n",
    "        audio_dir\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset.class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e5a6d-fa8c-4e45-b57b-c11fb96e618c",
   "metadata": {},
   "source": [
    "#### MINDS-14 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18780a3c-cb9a-4f54-8cec-07736ec0f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_minds14_local_dataset(dataset_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare MINDS-14 dataset for training from local files\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the downloaded dataset\n",
    "        batch_size: Batch size for dataloaders\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, intents, intent_to_idx\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = MINDS14Dataset(\n",
    "        dataset_path, \n",
    "        split='train', \n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset.intents, train_dataset.intent_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447d4f3-bbb6-47ab-9174-04975056e3ee",
   "metadata": {},
   "source": [
    "#### CatMeow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "304c2abb-6ab6-4e74-8dc7-2a6b257a1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cat_meow_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse CatMeow filename to extract metadata\n",
    "    Filename format: C_NNNNN_BB_SS_OOOOO_RXX, where:\n",
    "    * C = emission context (values: B = brushing; F = waiting for food; I: isolation)\n",
    "    * NNNNN = cat's unique ID\n",
    "    * BB = breed (values: MC = Maine Coon; EU: European Shorthair)\n",
    "    * SS = sex (values: FI = female, intact; FN: female, neutered; MI: male, intact; MN: male, neutered)\n",
    "    * OOOOO = cat owner's unique ID\n",
    "    * R = recording session (values: 1, 2 or 3)\n",
    "    * XX = vocalization counter (values: 01..99)\n",
    "    \"\"\"\n",
    "    # Get the first character of the filename as the context\n",
    "    if filename.startswith('B'):\n",
    "        context = 'brushing'\n",
    "    elif filename.startswith('F'):\n",
    "        context = 'waiting_for_food'\n",
    "    elif filename.startswith('I'):\n",
    "        context = 'isolation'\n",
    "    else:\n",
    "        # Default context if none of the expected ones\n",
    "        context = 'unknown'\n",
    "    \n",
    "    return {\n",
    "        'filename': filename,\n",
    "        'context': context\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34688e0e-8eb8-46de-8db4-06ffbc688e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_catmeow_dataset(audio_dir_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare the CatMeow dataset for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir_path (str): Path to the dataset root directory\n",
    "        batch_size (int): Batch size for data loaders\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, class_mapping\n",
    "    \"\"\"\n",
    "    audio_files = [f for f in os.listdir(audio_dir_path) if f.endswith('.wav')]\n",
    "    \n",
    "    # Parse metadata from filenames\n",
    "    metadata_list = []\n",
    "    for filename in audio_files:\n",
    "        parsed = parse_cat_meow_filename(filename)\n",
    "        if parsed:\n",
    "            metadata_list.append(parsed)\n",
    "            \n",
    "    metadata_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "    print(\"First few rows of metadata:\")\n",
    "    print(metadata_df.head())\n",
    "    \n",
    "    # Check if we have at least one file from each context\n",
    "    context_counts = metadata_df['context'].value_counts()\n",
    "    print(f\"Context counts: {context_counts}\")\n",
    "    \n",
    "    if len(context_counts) < 2:\n",
    "        print(\"Warning: Not enough different contexts for stratification!\")\n",
    "        # If we don't have enough contexts, just do a random split without stratification\n",
    "        train_meta, test_meta = train_test_split(\n",
    "            metadata_df, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_meta, val_meta = train_test_split(\n",
    "            train_meta, test_size=0.25, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # Save the full metadata\n",
    "        metadata_dir = os.path.join(os.path.dirname(audio_dir_path), 'metadata')\n",
    "        os.makedirs(metadata_dir, exist_ok=True)\n",
    "        metadata_df.to_csv(os.path.join(metadata_dir, 'CatMeow_metadata.csv'), index=False)\n",
    "        \n",
    "        # Create stratified splits\n",
    "        train_meta, test_meta = train_test_split(\n",
    "            metadata_df, test_size=0.2, random_state=42, stratify=metadata_df['context']\n",
    "        )\n",
    "        train_meta, val_meta = train_test_split(\n",
    "            train_meta, test_size=0.25, random_state=42, stratify=train_meta['context']\n",
    "        )  # 0.25 x 0.8 = 0.2 of total\n",
    "    \n",
    "    # Save splits\n",
    "    train_meta.to_csv(os.path.join(metadata_dir, 'train.csv'), index=False)\n",
    "    val_meta.to_csv(os.path.join(metadata_dir, 'val.csv'), index=False)\n",
    "    test_meta.to_csv(os.path.join(metadata_dir, 'test.csv'), index=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CatMeowDataset(\n",
    "        train_meta,\n",
    "        audio_dir_path,\n",
    "    )\n",
    "    \n",
    "    val_dataset = CatMeowDataset(\n",
    "        val_meta,\n",
    "        audio_dir_path\n",
    "    )\n",
    "    \n",
    "    test_dataset = CatMeowDataset(\n",
    "        test_meta,\n",
    "        audio_dir_path\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset.class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01feba40-d0d3-432c-9350-d2e149b10cf5",
   "metadata": {},
   "source": [
    "#### ESC-50 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7818fb20-a41f-4529-833c-a54170ed887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_esc50_dataset(esc50_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare the ESC-50 dataset for training, validation, and testing.\n",
    "    \n",
    "    Args:\n",
    "        esc50_path (str): Path to the ESC-50 dataset root directory\n",
    "        batch_size (int): Batch size for data loaders\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, class_mapping\n",
    "    \"\"\"\n",
    "    audio_dir = os.path.join(esc50_path, 'audio')\n",
    "    meta_dir = os.path.join(esc50_path, 'meta')\n",
    "    \n",
    "    metadata_path = os.path.join(meta_dir, 'esc50.csv')\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    required_columns = ['filename', 'category', 'fold']\n",
    "    missing_columns = [col for col in required_columns if col not in metadata_df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        if 'target' in metadata_df.columns and 'category' in missing_columns:\n",
    "            # map target numbers to category names if we have a target column but no category\n",
    "            metadata_df['category'] = metadata_df['target'].apply(lambda x: f'class_{x}')\n",
    "        else:\n",
    "            raise ValueError(f\"Metadata is missing required columns: {missing_columns}\")\n",
    "\n",
    "    if 'fold' in metadata_df.columns:\n",
    "        # Use folds 1-4 for training, fold 5 for testing (the standard ESC-50 split)\n",
    "        train_val_meta = metadata_df[metadata_df['fold'] <= 4].copy()\n",
    "        test_meta = metadata_df[metadata_df['fold'] == 5].copy()\n",
    "        \n",
    "        # Split training into train and validation\n",
    "        train_meta, val_meta = train_test_split(\n",
    "            train_val_meta, test_size=0.25, random_state=42, \n",
    "            stratify=train_val_meta['category']\n",
    "        )\n",
    "    else:\n",
    "        # If no fold information, do a standard split\n",
    "        train_meta, test_meta = train_test_split(\n",
    "            metadata_df, test_size=0.2, random_state=42, \n",
    "            stratify=metadata_df['category']\n",
    "        )\n",
    "        train_meta, val_meta = train_test_split(\n",
    "            train_meta, test_size=0.25, random_state=42, \n",
    "            stratify=train_meta['category']\n",
    "        )  # 0.25 x 0.8 = 0.2 of total\n",
    "\n",
    "    # save splits\n",
    "    os.makedirs(meta_dir, exist_ok=True)\n",
    "    train_meta.to_csv(os.path.join(meta_dir, 'train.csv'), index=False)\n",
    "    val_meta.to_csv(os.path.join(meta_dir, 'val.csv'), index=False)\n",
    "    test_meta.to_csv(os.path.join(meta_dir, 'test.csv'), index=False)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = ESC50Dataset(\n",
    "        train_meta,\n",
    "        audio_dir,\n",
    "    )\n",
    "    \n",
    "    val_dataset = ESC50Dataset(\n",
    "        val_meta,\n",
    "        audio_dir\n",
    "    )\n",
    "    \n",
    "    test_dataset = ESC50Dataset(\n",
    "        test_meta,\n",
    "        audio_dir\n",
    "    )\n",
    "    \n",
    "    # create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset.class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358cf82-4299-4f81-baeb-fc85faa2c6ad",
   "metadata": {},
   "source": [
    "### Some Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8dd06-5cbf-4685-a4c7-e9721a863f90",
   "metadata": {},
   "source": [
    "#### ESC-50 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1367bf2b-69a2-4cfd-998e-95cc57c7babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'airplane': 0, 'breathing': 1, 'brushing_teeth': 2, 'can_opening': 3, 'car_horn': 4, 'cat': 5, 'chainsaw': 6, 'chirping_birds': 7, 'church_bells': 8, 'clapping': 9, 'clock_alarm': 10, 'clock_tick': 11, 'coughing': 12, 'cow': 13, 'crackling_fire': 14, 'crickets': 15, 'crow': 16, 'crying_baby': 17, 'dog': 18, 'door_wood_creaks': 19, 'door_wood_knock': 20, 'drinking_sipping': 21, 'engine': 22, 'fireworks': 23, 'footsteps': 24, 'frog': 25, 'glass_breaking': 26, 'hand_saw': 27, 'helicopter': 28, 'hen': 29, 'insects': 30, 'keyboard_typing': 31, 'laughing': 32, 'mouse_click': 33, 'pig': 34, 'pouring_water': 35, 'rain': 36, 'rooster': 37, 'sea_waves': 38, 'sheep': 39, 'siren': 40, 'sneezing': 41, 'snoring': 42, 'thunderstorm': 43, 'toilet_flush': 44, 'train': 45, 'vacuum_cleaner': 46, 'washing_machine': 47, 'water_drops': 48, 'wind': 49}\n",
      "Number of classes: 50\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, class_mapping = prepare_esc50_dataset(path_to_ESC)\n",
    "print(f\"Class mapping: {class_mapping}\")\n",
    "print(f\"Number of classes: {len(class_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810174d-25b7-4ccb-8473-784307ed1a13",
   "metadata": {},
   "source": [
    "#### CatMeow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34ea84a7-c82a-4362-8c36-48309a0e4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of metadata:\n",
      "                      filename   context\n",
      "0  B_ANI01_MC_FN_SIM01_101.wav  brushing\n",
      "1  B_ANI01_MC_FN_SIM01_102.wav  brushing\n",
      "2  B_ANI01_MC_FN_SIM01_103.wav  brushing\n",
      "3  B_ANI01_MC_FN_SIM01_301.wav  brushing\n",
      "4  B_ANI01_MC_FN_SIM01_302.wav  brushing\n",
      "Context counts: context\n",
      "isolation           221\n",
      "brushing            127\n",
      "waiting_for_food     92\n",
      "Name: count, dtype: int64\n",
      "Class mapping: {'brushing': 0, 'isolation': 1, 'waiting_for_food': 2}\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, class_mapping = prepare_catmeow_dataset(path_to_Cat)\n",
    "print(f\"Class mapping: {class_mapping}\")\n",
    "print(f\"Number of classes: {len(class_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfa666-5368-413b-9f54-5d1ee6bb6b71",
   "metadata": {},
   "source": [
    "#### UrbanSound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad4cbe85-35be-4a5c-a60d-e51afbaded0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'air_conditioner': 0, 'car_horn': 1, 'children_playing': 2, 'dog_bark': 3, 'drilling': 4, 'engine_idling': 5, 'gun_shot': 6, 'jackhammer': 7, 'siren': 8, 'street_music': 9}\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, class_mapping = prepare_urbansound_dataset(path_to_UrbanSound8K)\n",
    "print(f\"Class mapping: {class_mapping}\")\n",
    "print(f\"Number of classes: {len(class_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dfd2a-685a-477f-98b5-122fee15a19f",
   "metadata": {},
   "source": [
    "#### MINDS-14 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5fb8d3d1-ac37-4d07-bfaf-497ea8b013e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 14\n",
      "Intent classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, intents, intent_to_idx = prepare_minds14_local_dataset(path_to_minds14)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Number of classes: {len(intents)}\")\n",
    "print(f\"Intent classes: {intents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161027b-e29a-4e9d-865a-1e3f3589070c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
