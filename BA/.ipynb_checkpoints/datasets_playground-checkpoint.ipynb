{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20031f06-e40a-4e52-91c2-2385ba908146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "data_folder_path = os.path.join(current_directory, 'data')\n",
    "datasets_path_file = os.path.join(data_folder_path, 'datasets_path.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d58b182c-448f-4fc9-bb1a-59e32ef910cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datasets_path_file, 'r', encoding='utf-8') as file:\n",
    "    datasets_path = json.load(file)\n",
    "\n",
    "path_to_UrbanSound8K = datasets_path.get(\"UrbanSound8K\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c421c95b-b8d4-4656-a4f5-a78bbdd33e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\" Function to bring audio files to the same length \"\"\"\n",
    "    waveforms, labels = zip(*batch)\n",
    "\n",
    "    max_length = max([w.shape[1] for w in waveforms])\n",
    "\n",
    "    # add zeros or cut off\n",
    "    padded_waveforms = []\n",
    "    for w in waveforms:\n",
    "        if w.shape[1] < max_length:\n",
    "            pad = torch.zeros((1, max_length - w.shape[1]))\n",
    "            padded_waveforms.append(torch.cat((w, pad), dim=1))\n",
    "        else:\n",
    "            padded_waveforms.append(w[:, :max_length])\n",
    "\n",
    "    return torch.stack(padded_waveforms), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad93ceed-c70c-4515-a2f0-eb7511fd6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSound8KDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata_file, folds, target_sample_rate=22050, target_length=22050):\n",
    "        self.data_dir = data_dir\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.target_length = target_length\n",
    "        metadata = pd.read_csv(metadata_file)\n",
    "        self.metadata = metadata[metadata['fold'].isin(folds)].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        file_path = os.path.join(self.data_dir, f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "        label = torch.tensor(row['classID'], dtype=torch.long)\n",
    "\n",
    "        # upload the audio file\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # convert to mono (if stereo)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0546d5ef-fc88-497e-a07c-6de05d56033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform shape: torch.Size([8, 1, 192000])\n",
      "Labels: tensor([7, 3, 8, 7, 3, 0, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(path_to_UrbanSound8K, 'audio')\n",
    "metadata_file = os.path.join(path_to_UrbanSound8K, 'metadata//UrbanSound8K.csv')\n",
    "\n",
    "train_folds = list(range(1, 10))\n",
    "test_folds = [10] \n",
    "\n",
    "train_dataset = UrbanSound8KDataset(data_dir, metadata_file, train_folds)\n",
    "test_dataset = UrbanSound8KDataset(data_dir, metadata_file, test_folds)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for batch in train_loader:\n",
    "    waveform, label = batch \n",
    "    print(\"Waveform shape:\", waveform.shape)\n",
    "    print(\"Labels:\", label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14e8c86c-467d-428b-81b1-6002ec86176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "class MelSpectrogramClassifier(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_mels=64, num_classes=10):\n",
    "        super(MelSpectrogramClassifier, self).__init__()\n",
    "\n",
    "        # audio → Mel Spectrogram\n",
    "        self.mel_spec = T.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "\n",
    "        # linear classifier\n",
    "        self.fc = nn.Linear(n_mels * 375, num_classes)  # 375 — фиксированное количество временных шагов\n",
    "\n",
    "    def forward(self, x):\n",
    "        # audio → Mel Spectrogram\n",
    "        x = self.mel_spec(x)  # (batch, 1, n_mels, time)\n",
    "\n",
    "        # remove 1-channel size\n",
    "        x = x.squeeze(1)  # (batch, n_mels, time)\n",
    "\n",
    "        # trim/fall to a fixed size (375 time-steps)\n",
    "        if x.shape[-1] > 375:\n",
    "            x = x[..., :375]\n",
    "        else:\n",
    "            pad_size = 375 - x.shape[-1]\n",
    "            x = torch.nn.functional.pad(x, (0, pad_size))\n",
    "\n",
    "        # expand into a vector\n",
    "        x = x.reshape(x.size(0), -1)  # (batch, features)\n",
    "\n",
    "        # classification\n",
    "        x = self.fc(x)  # (batch, num_classes)\n",
    "        return x\n",
    "\n",
    "# checking the model\n",
    "model = MelSpectrogramClassifier()\n",
    "dummy_waveform = torch.randn(8, 1, 192000)  # Твои данные\n",
    "output = model(dummy_waveform)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Ожидаемый размер (8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7db24d02-f3e8-4801-a8d2-d9eb2d065e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MelSpectrogramClassifier().to(device)\n",
    "\n",
    "# optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dcd74428-37b4-4bc2-920e-4605515d1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        for waveforms, labels in train_loader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        val_acc, val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in loader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    return acc, loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "44808cc7-de42-40d9-9074-ee2a0a3a95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 149.9836, Train Acc: 31.20%, Val Loss: 209.5145, Val Acc: 28.55%\n",
      "Epoch 2/10: Train Loss: 95.2716, Train Acc: 40.00%, Val Loss: 286.5601, Val Acc: 33.81%\n",
      "Epoch 3/10: Train Loss: 59.6150, Train Acc: 47.14%, Val Loss: 328.2712, Val Acc: 30.35%\n",
      "Epoch 4/10: Train Loss: 67.0583, Train Acc: 48.03%, Val Loss: 279.9898, Val Acc: 30.47%\n",
      "Epoch 5/10: Train Loss: 36.1193, Train Acc: 53.98%, Val Loss: 218.5443, Val Acc: 34.41%\n",
      "Epoch 6/10: Train Loss: 46.1874, Train Acc: 54.15%, Val Loss: 305.8724, Val Acc: 35.72%\n",
      "Epoch 7/10: Train Loss: 38.7649, Train Acc: 56.12%, Val Loss: 278.4018, Val Acc: 29.15%\n",
      "Epoch 8/10: Train Loss: 31.4592, Train Acc: 58.87%, Val Loss: 314.4935, Val Acc: 29.63%\n",
      "Epoch 9/10: Train Loss: 32.4565, Train Acc: 59.53%, Val Loss: 311.5493, Val Acc: 34.17%\n",
      "Epoch 10/10: Train Loss: 38.6307, Train Acc: 58.71%, Val Loss: 303.4810, Val Acc: 32.50%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, test_loader, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b0dcef31-18e0-4cc8-afd0-0ca993e44c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 32.50%\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate_model(model, test_loader, criterion)\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
